{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNSRuBaAnGPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7997be90-d280-43a6-ecdc-8c634daaac94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loguru\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.7.2\n"
          ]
        }
      ],
      "source": [
        "pip install loguru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOWE08AnnLQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3a7d78b-915b-45e9-ea53-d6fba39b8802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pendulum\n",
            "  Downloading pendulum-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6 in /usr/local/lib/python3.10/dist-packages (from pendulum) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pendulum) (2024.1)\n",
            "Collecting time-machine>=2.6.0 (from pendulum)\n",
            "  Downloading time_machine-2.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.6->pendulum) (1.16.0)\n",
            "Downloading pendulum-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading time_machine-2.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
            "Installing collected packages: time-machine, pendulum\n",
            "Successfully installed pendulum-3.0.0 time-machine-2.15.0\n"
          ]
        }
      ],
      "source": [
        "pip install pendulum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xziHkbffNw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef58a0e-4ebd-4fda-b93c-8d17ddfdec54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cufflinks in /usr/local/lib/python3.10/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cufflinks) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.10/dist-packages (from cufflinks) (2.1.4)\n",
            "Requirement already satisfied: plotly>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from cufflinks) (5.15.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from cufflinks) (1.16.0)\n",
            "Requirement already satisfied: colorlover>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from cufflinks) (0.3.0)\n",
            "Requirement already satisfied: setuptools>=34.4.1 in /usr/local/lib/python3.10/dist-packages (from cufflinks) (71.0.4)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from cufflinks) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from cufflinks) (7.7.1)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->cufflinks)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->cufflinks) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->cufflinks) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->cufflinks) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->cufflinks) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->cufflinks) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->cufflinks) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->cufflinks) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->cufflinks) (4.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->cufflinks) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->cufflinks) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->cufflinks) (3.6.8)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->cufflinks) (3.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.2->cufflinks) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.2->cufflinks) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.2->cufflinks) (2024.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.1.1->cufflinks) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.1.1->cufflinks) (24.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (6.3.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->cufflinks) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->cufflinks) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->cufflinks) (0.2.13)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (6.5.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (1.1.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (4.23.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (0.19.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->cufflinks) (1.2.2)\n",
            "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n"
          ]
        }
      ],
      "source": [
        "pip install cufflinks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjNqvJtOEBiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a457344-a698-4843-d88f-3939ac997975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nru24nxgCyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295e8753-00a5-4ddb-e9f8-96c8b141516c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n"
          ]
        }
      ],
      "source": [
        "pip install lightgbm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_QGQWqnDQvh"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiCZlV5xCcgF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "a1ad0efb-337c-4cc4-d56b-a1b049a8d115"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning:\n",
            "\n",
            "\n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import pprint\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "init_notebook_mode(connected=True)\n",
        "import cufflinks as cf\n",
        "cf.go_offline()\n",
        "import pickle\n",
        "import gc\n",
        "import lightgbm as lgb\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "from loguru import logger\n",
        "import pendulum\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcgQ_evGnCCa"
      },
      "outputs": [],
      "source": [
        "# Set logging format\n",
        "log_fmt = \"<green>{time:YYYY-MM-DD HH:mm:ss.SSS!UTC}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - {message}\"\n",
        "logger.configure(handlers=[{\"sink\": sys.stderr, \"format\": log_fmt}])\n",
        "\n",
        "# current date\n",
        "CURRENT_DATE = pendulum.now(tz=\"UTC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_cfSu7L35ho"
      },
      "outputs": [],
      "source": [
        "DIR = \"/content/drive/MyDrive/home-credit-default-risk/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CLEANED_DATA_DIR = \"/content/drive/MyDrive/cleaned_data/\""
      ],
      "metadata": {
        "id": "mQmcWZeKDLh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from loguru import logger\n",
        "\n",
        "def reduce_memory_usage(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Optimize the memory usage of a DataFrame by downcasting numerical columns to more efficient types.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame for which memory usage should be optimized.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with optimized memory usage.\n",
        "\n",
        "    Notes:\n",
        "        - This function will downcast integer columns to the smallest possible integer type (int8, int16, int32, or int64)\n",
        "          based on their minimum and maximum values.\n",
        "        - It will also downcast floating-point columns to the smallest possible floating-point type (float16, float32, or float64).\n",
        "        - Columns of type object (e.g., strings) are not modified.\n",
        "        - The function prints the memory usage before and after optimization and the percentage decrease in memory usage.\n",
        "    \"\"\"\n",
        "    # Calculate and print initial memory usage\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    logger.info('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "\n",
        "    # Iterate through each column to optimize its data type\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                # Downcast integer columns\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                # Downcast float columns\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    # Calculate and print memory usage after optimization\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    logger.info('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    logger.info('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "H0GqVIGml4ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nt4A352nfdM"
      },
      "source": [
        "# Preprocessing of each dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hToMBAmHDB41"
      },
      "source": [
        "## Bureau and Bureau_balance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from loguru import logger\n",
        "import pendulum\n",
        "from typing import Optional, Union\n",
        "\n",
        "class preprocess_bureau_balance_and_bureau:\n",
        "    '''\n",
        "    Preprocess the tables bureau_balance and bureau.\n",
        "    Contains 4 member functions:\n",
        "        1. init method\n",
        "        2. preprocess_bureau_balance method\n",
        "        3. preprocess_bureau method\n",
        "        4. main method\n",
        "    '''\n",
        "\n",
        "    def __init__(self, file_directory: str = '', verbose: bool = True, dump_to_pickle: bool = False):\n",
        "        '''\n",
        "        This function is used to initialize the class members\n",
        "\n",
        "        Inputs:\n",
        "            self\n",
        "            file_directory: Path, str, default = ''\n",
        "                The path where the file exists. Include a '/' at the end of the path in input\n",
        "            verbose: bool, default = True\n",
        "                Whether to enable verbosity or not\n",
        "            dump_to_pickle: bool, default = False\n",
        "                Whether to pickle the final preprocessed table or not\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "\n",
        "        self.file_directory = file_directory\n",
        "        self.verbose = verbose\n",
        "        self.dump_to_pickle = dump_to_pickle\n",
        "        self.start = datetime.now()\n",
        "        logger.info('Preprocessing class initialized.')\n",
        "\n",
        "    def preprocess_bureau_balance(self):\n",
        "        '''\n",
        "        Function to preprocess bureau_balance table.\n",
        "        This function first loads the table into memory, does some feature engineering, and finally\n",
        "        aggregates the data over SK_ID_BUREAU\n",
        "\n",
        "        Inputs:\n",
        "            self\n",
        "\n",
        "        Returns:\n",
        "            preprocessed and aggregated bureau_balance table.\n",
        "        '''\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info('#######################################################')\n",
        "            logger.info('#          Pre-processing bureau_balance.csv          #')\n",
        "            logger.info('#######################################################')\n",
        "            logger.info(\"\\nLoading the DataFrame, bureau_balance.csv, into memory...\")\n",
        "\n",
        "        bureau_balance = reduce_memory_usage(pd.read_csv(self.file_directory + 'bureau_balance.csv'))\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Loaded bureau_balance.csv\")\n",
        "            logger.info(f\"Time Taken to load = {datetime.now() - self.start}\")\n",
        "            logger.info(\"\\nStarting Data Cleaning and Feature Engineering...\")\n",
        "\n",
        "        # # Encode STATUS labels\n",
        "        # dict_for_status = {'C': 0, '0': 1, '1': 2, '2': 3, 'X': 4, '3': 5, '4': 6, '5': 7}\n",
        "        # bureau_balance['STATUS'] = bureau_balance['STATUS'].map(dict_for_status)\n",
        "\n",
        "        # if self.verbose:\n",
        "        #     logger.info(\"Halfway through. A little bit more patience...\")\n",
        "        #     logger.info(f\"Total Time Elapsed = {datetime.now() - self.start}\")\n",
        "\n",
        "        # # Aggregating over whole dataset\n",
        "        # aggregated_bureau_balance = bureau_balance.groupby(['SK_ID_BUREAU']).mean().reset_index()\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info('Done preprocessing bureau_balance.')\n",
        "            logger.info(f\"\\nInitial Size of bureau_balance: {bureau_balance.shape}\")\n",
        "            logger.info(f'Size of bureau_balance after Pre-Processing, Feature Engineering and Aggregation: {bureau_balance.shape}')\n",
        "            logger.info(f'\\nTotal Time Taken = {datetime.now() - self.start}')\n",
        "\n",
        "        if self.dump_to_pickle:\n",
        "            if self.verbose:\n",
        "                logger.info('\\nPickling pre-processed bureau_balance to bureau_balance_preprocessed.pkl')\n",
        "            with open(self.file_directory + 'bureau_balance_preprocessed.pkl', 'wb') as f:\n",
        "                pickle.dump(aggregated_bureau_balance, f)\n",
        "            if self.verbose:\n",
        "                logger.info('Done.')\n",
        "\n",
        "        return bureau_balance\n",
        "\n",
        "    def preprocess_bureau(self, aggregated_bureau_balance):\n",
        "        '''\n",
        "        Function to preprocess the bureau table and merge it with the aggregated bureau_balance table.\n",
        "\n",
        "        Inputs:\n",
        "            self\n",
        "            aggregated_bureau_balance: DataFrame of aggregated bureau_balance table\n",
        "\n",
        "        Returns:\n",
        "            Final preprocessed, merged and aggregated bureau table\n",
        "        '''\n",
        "\n",
        "        if self.verbose:\n",
        "            start2 = datetime.now()\n",
        "            logger.info('\\n##############################################')\n",
        "            logger.info('#          Pre-processing bureau.csv         #')\n",
        "            logger.info('##############################################')\n",
        "            if self.verbose:\n",
        "                start2 = datetime.now()\n",
        "                logger.info('Starting preprocessing of bureau.csv')\n",
        "            logger.info(\"\\nLoading the DataFrame, bureau.csv, into memory...\")\n",
        "\n",
        "        bureau = reduce_memory_usage(pd.read_csv(self.file_directory + 'bureau.csv'))\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Loaded bureau.csv\")\n",
        "            logger.info(f\"Time Taken to load = {datetime.now() - start2}\")\n",
        "            logger.info(\"\\nStarting Data Cleaning and Feature Engineering...\")\n",
        "        # Merge with aggregated_bureau_balance\n",
        "        bureau_merged = bureau.merge(aggregated_bureau_balance, on=['SK_ID_BUREAU'], how='right').drop('SK_ID_BUREAU', axis=1)\n",
        "        # Combine numerical features\n",
        "        bureau_numerical_aggregated = bureau_merged.select_dtypes(include=[np.number]).groupby(['SK_ID_CURR']).mean().reset_index()\n",
        "        bureau_numerical_aggregated.columns = ['BUREAU_' + column if column != 'SK_ID_CURR' else column for column in bureau_numerical_aggregated.columns]\n",
        "\n",
        "        # Combine categorical features\n",
        "        bureau_categorical = pd.get_dummies(bureau_merged.select_dtypes('object'))\n",
        "        bureau_categorical['SK_ID_CURR'] = bureau['SK_ID_CURR']\n",
        "        bureau_categorical_aggregated = bureau_categorical.groupby(['SK_ID_CURR']).mean().reset_index()\n",
        "\n",
        "\n",
        "        # Merge numerical and categorical features\n",
        "        bureau_merged_aggregated = bureau_numerical_aggregated.merge(bureau_categorical_aggregated, on='SK_ID_CURR')\n",
        "        bureau_merged_aggregated.update(bureau_merged_aggregated.fillna(0))\n",
        "        bureau_merged_aggregated.columns = ['BUREAU_' + column if column != 'SK_ID_CURR' else column for column in bureau_merged_aggregated.columns]\n",
        "\n",
        "        bureau_merged_aggregated.fillna(0, inplace=True)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info('Preprocessing of bureau completed.')\n",
        "            logger.info('Initial Size of bureau: {}', bureau.shape)\n",
        "            logger.info('Size after merging, preprocessing, and aggregation: {}', bureau_merged_aggregated.shape)\n",
        "            logger.info('Total Time Taken: {}', datetime.now() - self.start)\n",
        "\n",
        "        if self.dump_to_pickle:\n",
        "            if self.verbose:\n",
        "                logger.info('Pickling pre-processed bureau and bureau_balance to bureau_merged_preprocessed.pkl')\n",
        "            with open(self.file_directory + 'bureau_merged_preprocessed.pkl', 'wb') as f:\n",
        "                pickle.dump(bureau_merged_aggregated, f)\n",
        "            if self.verbose:\n",
        "                logger.info('Pickling completed.')\n",
        "        if self.verbose:\n",
        "            logger.info('-' * 100)\n",
        "\n",
        "        return bureau_merged_aggregated\n",
        "\n",
        "    def main(self) -> pd.DataFrame:\n",
        "        '''\n",
        "        Function to be called for complete preprocessing and aggregation of the bureau and bureau_balance tables.\n",
        "\n",
        "        Inputs:\n",
        "            self\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The final preprocessed and merged `bureau` and `bureau_balance` tables.\n",
        "        '''\n",
        "\n",
        "        # Preprocess the bureau_balance first\n",
        "        aggregated_bureau_balance = self.preprocess_bureau_balance()\n",
        "\n",
        "        # Preprocess the bureau table next, by combining it with the aggregated bureau_balance\n",
        "        bureau_merged_aggregated = self.preprocess_bureau(aggregated_bureau_balance)\n",
        "\n",
        "\n",
        "        return bureau_merged_aggregated\n"
      ],
      "metadata": {
        "id": "vf4gTzBdLSRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4fXTWrkGCia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e80ba1f-17c3-45ed-e2b0-0ca43f80b62b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2024-08-06 22:33:27.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - Preprocessing class initialized.\n",
            "\u001b[32m2024-08-06 22:33:27.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau_balance\u001b[0m:\u001b[36m55\u001b[0m - #######################################################\n",
            "\u001b[32m2024-08-06 22:33:27.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau_balance\u001b[0m:\u001b[36m56\u001b[0m - #          Pre-processing bureau_balance.csv          #\n",
            "\u001b[32m2024-08-06 22:33:27.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau_balance\u001b[0m:\u001b[36m57\u001b[0m - #######################################################\n",
            "\u001b[32m2024-08-06 22:33:27.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau_balance\u001b[0m:\u001b[36m58\u001b[0m - \n",
            "Loading the DataFrame, bureau_balance.csv, into memory...\n",
            "\u001b[32m2024-08-06 22:33:47.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m24\u001b[0m - Memory usage of dataframe is 624.85 MB\n",
            "\u001b[32m2024-08-06 22:33:47.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m55\u001b[0m - Memory usage after optimization is: 338.46 MB\n",
            "\u001b[32m2024-08-06 22:33:47.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m56\u001b[0m - Decreased by 45.8%\n",
            "\u001b[32m2024-08-06 22:33:47.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau_balance\u001b[0m:\u001b[36m63\u001b[0m - Loaded bureau_balance.csv\n",
            "\u001b[32m2024-08-06 22:33:47.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau_balance\u001b[0m:\u001b[36m64\u001b[0m - Time Taken to load = 0:00:20.523276\n",
            "\u001b[32m2024-08-06 22:33:47.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau_balance\u001b[0m:\u001b[36m65\u001b[0m - \n",
            "Starting Data Cleaning and Feature Engineering...\n",
            "\u001b[32m2024-08-06 22:33:47.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau_balance\u001b[0m:\u001b[36m79\u001b[0m - Done preprocessing bureau_balance.\n",
            "\u001b[32m2024-08-06 22:33:47.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau_balance\u001b[0m:\u001b[36m80\u001b[0m - \n",
            "Initial Size of bureau_balance: (27299925, 3)\n",
            "\u001b[32m2024-08-06 22:33:47.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau_balance\u001b[0m:\u001b[36m81\u001b[0m - Size of bureau_balance after Pre-Processing, Feature Engineering and Aggregation: (27299925, 3)\n",
            "\u001b[32m2024-08-06 22:33:47.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau_balance\u001b[0m:\u001b[36m82\u001b[0m - \n",
            "Total Time Taken = 0:00:20.530662\n",
            "\u001b[32m2024-08-06 22:33:47.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau\u001b[0m:\u001b[36m108\u001b[0m - \n",
            "##############################################\n",
            "\u001b[32m2024-08-06 22:33:47.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau\u001b[0m:\u001b[36m109\u001b[0m - #          Pre-processing bureau.csv         #\n",
            "\u001b[32m2024-08-06 22:33:47.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau\u001b[0m:\u001b[36m110\u001b[0m - ##############################################\n",
            "\u001b[32m2024-08-06 22:33:47.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau\u001b[0m:\u001b[36m113\u001b[0m - Starting preprocessing of bureau.csv\n",
            "\u001b[32m2024-08-06 22:33:47.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau\u001b[0m:\u001b[36m114\u001b[0m - \n",
            "Loading the DataFrame, bureau.csv, into memory...\n",
            "\u001b[32m2024-08-06 22:33:53.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m24\u001b[0m - Memory usage of dataframe is 222.62 MB\n",
            "\u001b[32m2024-08-06 22:33:54.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m55\u001b[0m - Memory usage after optimization is: 112.95 MB\n",
            "\u001b[32m2024-08-06 22:33:54.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m56\u001b[0m - Decreased by 49.3%\n",
            "\u001b[32m2024-08-06 22:33:54.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau\u001b[0m:\u001b[36m119\u001b[0m - Loaded bureau.csv\n",
            "\u001b[32m2024-08-06 22:33:54.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau\u001b[0m:\u001b[36m120\u001b[0m - Time Taken to load = 0:00:06.460231\n",
            "\u001b[32m2024-08-06 22:33:54.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau\u001b[0m:\u001b[36m121\u001b[0m - \n",
            "Starting Data Cleaning and Feature Engineering...\n",
            "\u001b[32m2024-08-06 22:34:44.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau\u001b[0m:\u001b[36m142\u001b[0m - Preprocessing of bureau completed.\n",
            "\u001b[32m2024-08-06 22:34:44.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau\u001b[0m:\u001b[36m143\u001b[0m - Initial Size of bureau: (1716428, 17)\n",
            "\u001b[32m2024-08-06 22:34:44.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau\u001b[0m:\u001b[36m144\u001b[0m - Size after merging, preprocessing, and aggregation: (134542, 44)\n",
            "\u001b[32m2024-08-06 22:34:44.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau\u001b[0m:\u001b[36m145\u001b[0m - Total Time Taken: 0:01:17.484984\n",
            "\u001b[32m2024-08-06 22:34:44.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess_bureau\u001b[0m:\u001b[36m155\u001b[0m - ----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "bureau_aggregated = preprocess_bureau_balance_and_bureau(file_directory=DIR).main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zjUMUx3KFrr"
      },
      "source": [
        "## Previous Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d3Tn-BuKJpM"
      },
      "outputs": [],
      "source": [
        "class preprocess_previous_application:\n",
        "    '''\n",
        "    Preprocess the previous_application table.\n",
        "\n",
        "    This class contains methods to load, clean, preprocess, and aggregate the `previous_application` table.\n",
        "\n",
        "    Attributes:\n",
        "        file_directory (str): Path to the directory containing the data files.\n",
        "        verbose (bool): Whether to enable verbose logging.\n",
        "        dump_to_pickle (bool): Whether to pickle the final preprocessed table.\n",
        "        nrows (Optional[int]): Number of rows to read from the CSV file.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, file_directory: str = '', verbose: bool = True, dump_to_pickle: bool = False, nrows: Optional[int] = None):\n",
        "        '''\n",
        "        Initializes the preprocess_previous_application class.\n",
        "\n",
        "        Args:\n",
        "            file_directory (str): Path to the directory where the files are located.\n",
        "            verbose (bool): Whether to enable verbose logging.\n",
        "            dump_to_pickle (bool): Whether to pickle the final preprocessed table.\n",
        "            nrows (Optional[int]): Number of rows to read from the CSV file.\n",
        "        '''\n",
        "        self.file_directory = file_directory\n",
        "        self.verbose = verbose\n",
        "        self.dump_to_pickle = dump_to_pickle\n",
        "        self.nrows = nrows\n",
        "        self.start = datetime.now()\n",
        "        logger.info('Preprocessing class initialized.')\n",
        "\n",
        "    def load_dataframe(self):\n",
        "        '''\n",
        "        Loads the `previous_application.csv` DataFrame into memory.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            logger.info('########################################################')\n",
        "            logger.info('#        Pre-processing previous_application.csv        #')\n",
        "            logger.info('########################################################')\n",
        "            logger.info(\"Loading the DataFrame, previous_application.csv, into memory...\")\n",
        "\n",
        "        # Loading the DataFrame into memory\n",
        "        self.previous_application = reduce_memory_usage(pd.read_csv(self.file_directory + 'previous_application.csv', nrows=self.nrows))\n",
        "        self.initial_shape = self.previous_application.shape\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Loaded previous_application.csv\")\n",
        "            logger.info('Time Taken to load: {}', datetime.now() - self.start)\n",
        "\n",
        "    def data_cleaning(self):\n",
        "        '''\n",
        "        Cleans the data by removing erroneous points and filling categorical NaNs with 'XNA'.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            start = datetime.now()\n",
        "            logger.info('Starting Data Cleaning...')\n",
        "\n",
        "        # Example of data cleaning (customize as needed)\n",
        "        # self.previous_application['column_name'].fillna('XNA', inplace=True)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Data Cleaning Done.\")\n",
        "            logger.info('Time taken: {}', datetime.now() - start)\n",
        "\n",
        "    def preprocessing_feature_engineering(self):\n",
        "        '''\n",
        "        Performs preprocessing such as categorical encoding and feature engineering.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            start = datetime.now()\n",
        "            logger.info(\"Performing Preprocessing and Feature Engineering...\")\n",
        "\n",
        "        # Example of feature engineering (customize as needed)\n",
        "        # self.previous_application['new_feature'] = self.previous_application['feature_1'] / self.previous_application['feature_2']\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Preprocessing and Feature Engineering Done.\")\n",
        "            logger.info('Time taken: {}', datetime.now() - start)\n",
        "\n",
        "    def aggregations(self) -> pd.DataFrame:\n",
        "        '''\n",
        "        Aggregates the previous applications over `SK_ID_CURR` and merges with application_bureau.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Final DataFrame after merging and aggregations.\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            logger.info(\"Aggregating previous applications over SK_ID_CURR...\")\n",
        "\n",
        "        # Number of previous applications per customer\n",
        "        grp = self.previous_application[['SK_ID_CURR', 'SK_ID_PREV']].groupby(by=['SK_ID_CURR'])['SK_ID_PREV'].count().reset_index().rename(columns={'SK_ID_PREV': 'PREV_APP_COUNT'})\n",
        "        self.previous_application = self.previous_application.merge(grp, on=['SK_ID_CURR'], how='right')\n",
        "\n",
        "        # Combining numerical features\n",
        "        previous_numerical_aggregated = self.previous_application.select_dtypes(include=[np.number]).drop('SK_ID_PREV', axis=1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n",
        "\n",
        "        # Combining categorical features\n",
        "        previous_categorical = pd.get_dummies(self.previous_application.select_dtypes('object'))\n",
        "        previous_categorical['SK_ID_CURR'] = self.previous_application['SK_ID_CURR']\n",
        "        previous_categorical_aggregated = previous_categorical.groupby('SK_ID_CURR').mean().reset_index()\n",
        "\n",
        "        # Merge numerical and categorical features\n",
        "        previous_aggregated = previous_numerical_aggregated.merge(previous_categorical_aggregated, on='SK_ID_CURR')\n",
        "        previous_aggregated.columns = ['PREV_' + column if column != 'SK_ID_CURR' else column for column in previous_aggregated.columns]\n",
        "        previous_aggregated.fillna(0, inplace=True)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info('Aggregations Done.')\n",
        "            logger.info('Size after merging, preprocessing, and aggregation: {}', previous_aggregated.shape)\n",
        "\n",
        "        return previous_aggregated\n",
        "\n",
        "    def main(self) -> pd.DataFrame:\n",
        "        '''\n",
        "        Performs complete preprocessing and aggregation of the `previous_application` table.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Final preprocessed and aggregated `previous_application` table.\n",
        "        '''\n",
        "        # Loading the DataFrame\n",
        "        self.load_dataframe()\n",
        "\n",
        "        # Cleaning the data\n",
        "        self.data_cleaning()\n",
        "\n",
        "        # Preprocessing the categorical features and creating new features\n",
        "        self.preprocessing_feature_engineering()\n",
        "\n",
        "        # Aggregating data over SK_ID_CURR and merging with application_bureau\n",
        "        previous_aggregated = self.aggregations()\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info('Done aggregations.')\n",
        "            logger.info('Initial Size of previous_application: {}', self.initial_shape)\n",
        "            logger.info('Size of previous_application after Pre-Processing, Feature Engineering and Aggregation: {}', previous_aggregated.shape)\n",
        "            logger.info('Total Time Taken: {}', datetime.now() - self.start)\n",
        "\n",
        "        if self.dump_to_pickle:\n",
        "            if self.verbose:\n",
        "                logger.info('Pickling pre-processed previous_application to previous_application_preprocessed.pkl')\n",
        "            with open(self.file_directory + 'previous_application_preprocessed.pkl', 'wb') as f:\n",
        "                pickle.dump(previous_aggregated, f)\n",
        "            if self.verbose:\n",
        "                logger.info('Pickling completed.')\n",
        "\n",
        "        return previous_aggregated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "200v9awsKZqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "376defc9-3e3b-4647-902b-db5d309d727f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2024-08-06 22:34:45.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m29\u001b[0m - Preprocessing class initialized.\n",
            "\u001b[32m2024-08-06 22:34:45.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m39\u001b[0m - ########################################################\n",
            "\u001b[32m2024-08-06 22:34:45.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m40\u001b[0m - #        Pre-processing previous_application.csv        #\n",
            "\u001b[32m2024-08-06 22:34:45.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m41\u001b[0m - ########################################################\n",
            "\u001b[32m2024-08-06 22:34:45.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m42\u001b[0m - Loading the DataFrame, previous_application.csv, into memory...\n",
            "\u001b[32m2024-08-06 22:34:57.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m24\u001b[0m - Memory usage of dataframe is 471.48 MB\n",
            "\u001b[32m2024-08-06 22:34:58.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m55\u001b[0m - Memory usage after optimization is: 293.08 MB\n",
            "\u001b[32m2024-08-06 22:34:58.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m56\u001b[0m - Decreased by 37.8%\n",
            "\u001b[32m2024-08-06 22:34:58.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m49\u001b[0m - Loaded previous_application.csv\n",
            "\u001b[32m2024-08-06 22:34:58.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m50\u001b[0m - Time Taken to load: 0:00:12.970672\n",
            "\u001b[32m2024-08-06 22:34:58.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_cleaning\u001b[0m:\u001b[36m61\u001b[0m - Starting Data Cleaning...\n",
            "\u001b[32m2024-08-06 22:34:58.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_cleaning\u001b[0m:\u001b[36m67\u001b[0m - Data Cleaning Done.\n",
            "\u001b[32m2024-08-06 22:34:58.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_cleaning\u001b[0m:\u001b[36m68\u001b[0m - Time taken: 0:00:00.003989\n",
            "\u001b[32m2024-08-06 22:34:58.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocessing_feature_engineering\u001b[0m:\u001b[36m79\u001b[0m - Performing Preprocessing and Feature Engineering...\n",
            "\u001b[32m2024-08-06 22:34:58.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocessing_feature_engineering\u001b[0m:\u001b[36m85\u001b[0m - Preprocessing and Feature Engineering Done.\n",
            "\u001b[32m2024-08-06 22:34:58.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocessing_feature_engineering\u001b[0m:\u001b[36m86\u001b[0m - Time taken: 0:00:00.002934\n",
            "\u001b[32m2024-08-06 22:34:58.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36maggregations\u001b[0m:\u001b[36m96\u001b[0m - Aggregating previous applications over SK_ID_CURR...\n",
            "\u001b[32m2024-08-06 22:35:10.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36maggregations\u001b[0m:\u001b[36m116\u001b[0m - Aggregations Done.\n",
            "\u001b[32m2024-08-06 22:35:10.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36maggregations\u001b[0m:\u001b[36m117\u001b[0m - Size after merging, preprocessing, and aggregation: (338857, 164)\n",
            "\u001b[32m2024-08-06 22:35:10.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m141\u001b[0m - Done aggregations.\n",
            "\u001b[32m2024-08-06 22:35:10.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m142\u001b[0m - Initial Size of previous_application: (1670214, 37)\n",
            "\u001b[32m2024-08-06 22:35:10.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m143\u001b[0m - Size of previous_application after Pre-Processing, Feature Engineering and Aggregation: (338857, 164)\n",
            "\u001b[32m2024-08-06 22:35:10.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m144\u001b[0m - Total Time Taken: 0:00:25.762518\n"
          ]
        }
      ],
      "source": [
        "previous_aggregated = preprocess_previous_application(file_directory=CLEANED_DATA_DIR).main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txK6VSh9K9AD"
      },
      "source": [
        "## installments_payments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_AWxseuLPUz"
      },
      "outputs": [],
      "source": [
        "class preprocess_installments_payments:\n",
        "    '''\n",
        "    Preprocess the installments_payments table.\n",
        "\n",
        "    This class contains methods to load, preprocess, and aggregate the `installments_payments` table.\n",
        "\n",
        "    Attributes:\n",
        "        file_directory (str): Path to the directory containing the data files.\n",
        "        verbose (bool): Whether to enable verbose logging.\n",
        "        dump_to_pickle (bool): Whether to pickle the final preprocessed table.\n",
        "        nrows (Optional[int]): Number of rows to read from the CSV file.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, file_directory: str = '', verbose: bool = True, dump_to_pickle: bool = False, nrows: Optional[int] = None):\n",
        "        '''\n",
        "        Initializes the preprocess_installments_payments class.\n",
        "\n",
        "        Args:\n",
        "            file_directory (str): Path to the directory where the files are located.\n",
        "            verbose (bool): Whether to enable verbose logging.\n",
        "            dump_to_pickle (bool): Whether to pickle the final preprocessed table.\n",
        "            nrows (Optional[int]): Number of rows to read from the CSV file.\n",
        "        '''\n",
        "        self.file_directory = file_directory\n",
        "        self.verbose = verbose\n",
        "        self.dump_to_pickle = dump_to_pickle\n",
        "        self.nrows = nrows\n",
        "        self.start = datetime.now()\n",
        "        logger.info('Preprocessing class initialized.')\n",
        "\n",
        "    def load_dataframe(self):\n",
        "        '''\n",
        "        Loads the `installments_payments.csv` DataFrame into memory.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            logger.info('##########################################################')\n",
        "            logger.info('#        Pre-processing installments_payments.csv        #')\n",
        "            logger.info('##########################################################')\n",
        "            logger.info(\"Loading the DataFrame, installments_payments.csv, into memory...\")\n",
        "\n",
        "        self.installments_payments = reduce_memory_usage(pd.read_csv(self.file_directory + 'installments_payments.csv', nrows=self.nrows))\n",
        "        self.initial_shape = self.installments_payments.shape\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Loaded installments_payments.csv\")\n",
        "            logger.info('Time Taken to load: {}', datetime.now() - self.start)\n",
        "\n",
        "    def data_preprocessing_and_feature_engineering(self):\n",
        "        '''\n",
        "        Performs preprocessing and feature engineering on the DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            start = datetime.now()\n",
        "            logger.info(\"Starting Data Pre-processing and Feature Engineering...\")\n",
        "\n",
        "        # Example of preprocessing and feature engineering (customize as needed)\n",
        "        # self.installments_payments['new_feature'] = self.installments_payments['feature_1'] / self.installments_payments['feature_2']\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Data Pre-processing and Feature Engineering Done.\")\n",
        "            logger.info('Time Taken: {}', datetime.now() - start)\n",
        "\n",
        "    def aggregations_sk_id_curr(self) -> pd.DataFrame:\n",
        "        '''\n",
        "        Aggregates the installments payments on previous loans over SK_ID_CURR.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Installments payments aggregated over SK_ID_CURR.\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            logger.info(\"Aggregating installments payments over SK_ID_CURR...\")\n",
        "\n",
        "        # Combining numerical features (only numerical features)\n",
        "        installments_payments_aggregated = self.installments_payments.select_dtypes(include=[np.number]).drop('SK_ID_PREV', axis=1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n",
        "        installments_payments_aggregated.columns = ['INSTA_' + column if column != 'SK_ID_CURR' else column for column in installments_payments_aggregated.columns]\n",
        "        installments_payments_aggregated.fillna(0, inplace=True)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info('Aggregation Done.')\n",
        "            logger.info('Size after aggregation: {}', installments_payments_aggregated.shape)\n",
        "\n",
        "        return installments_payments_aggregated\n",
        "\n",
        "    def main(self) -> pd.DataFrame:\n",
        "        '''\n",
        "        Performs complete preprocessing and aggregation of the `installments_payments` table.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Final preprocessed and aggregated `installments_payments` table.\n",
        "        '''\n",
        "        # Loading the DataFrame\n",
        "        self.load_dataframe()\n",
        "\n",
        "        # Performing preprocessing and feature engineering\n",
        "        self.data_preprocessing_and_feature_engineering()\n",
        "\n",
        "        # Aggregating the installments payments over SK_ID_CURR\n",
        "        installments_payments_aggregated = self.aggregations_sk_id_curr()\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info('Done preprocessing installments_payments.')\n",
        "            logger.info('Initial Size of installments_payments: {}', self.initial_shape)\n",
        "            logger.info('Size of installments_payments after Pre-Processing, Feature Engineering and Aggregation: {}', installments_payments_aggregated.shape)\n",
        "            logger.info('Total Time Taken: {}', datetime.now() - self.start)\n",
        "\n",
        "        if self.dump_to_pickle:\n",
        "            if self.verbose:\n",
        "                logger.info('Pickling pre-processed installments_payments to installments_payments_preprocessed.pkl')\n",
        "            with open(self.file_directory + 'installments_payments_preprocessed.pkl', 'wb') as f:\n",
        "                pickle.dump(installments_payments_aggregated, f)\n",
        "            if self.verbose:\n",
        "                logger.info('Pickling completed.')\n",
        "\n",
        "        return installments_payments_aggregated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsQYrZcxLYEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b08f746-1411-433e-ab60-49bba334a810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2024-08-06 22:35:11.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m29\u001b[0m - Preprocessing class initialized.\n",
            "\u001b[32m2024-08-06 22:35:11.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m39\u001b[0m - ##########################################################\n",
            "\u001b[32m2024-08-06 22:35:11.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m40\u001b[0m - #        Pre-processing installments_payments.csv        #\n",
            "\u001b[32m2024-08-06 22:35:11.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m41\u001b[0m - ##########################################################\n",
            "\u001b[32m2024-08-06 22:35:11.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m42\u001b[0m - Loading the DataFrame, installments_payments.csv, into memory...\n",
            "\u001b[32m2024-08-06 22:35:28.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m24\u001b[0m - Memory usage of dataframe is 830.41 MB\n",
            "\u001b[32m2024-08-06 22:35:30.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m55\u001b[0m - Memory usage after optimization is: 311.40 MB\n",
            "\u001b[32m2024-08-06 22:35:30.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m56\u001b[0m - Decreased by 62.5%\n",
            "\u001b[32m2024-08-06 22:35:30.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m48\u001b[0m - Loaded installments_payments.csv\n",
            "\u001b[32m2024-08-06 22:35:30.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m49\u001b[0m - Time Taken to load: 0:00:19.025965\n",
            "\u001b[32m2024-08-06 22:35:30.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_preprocessing_and_feature_engineering\u001b[0m:\u001b[36m60\u001b[0m - Starting Data Pre-processing and Feature Engineering...\n",
            "\u001b[32m2024-08-06 22:35:30.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_preprocessing_and_feature_engineering\u001b[0m:\u001b[36m66\u001b[0m - Data Pre-processing and Feature Engineering Done.\n",
            "\u001b[32m2024-08-06 22:35:30.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_preprocessing_and_feature_engineering\u001b[0m:\u001b[36m67\u001b[0m - Time Taken: 0:00:00.003820\n",
            "\u001b[32m2024-08-06 22:35:30.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36maggregations_sk_id_curr\u001b[0m:\u001b[36m77\u001b[0m - Aggregating installments payments over SK_ID_CURR...\n",
            "\u001b[32m2024-08-06 22:35:32.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36maggregations_sk_id_curr\u001b[0m:\u001b[36m85\u001b[0m - Aggregation Done.\n",
            "\u001b[32m2024-08-06 22:35:32.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36maggregations_sk_id_curr\u001b[0m:\u001b[36m86\u001b[0m - Size after aggregation: (339587, 7)\n",
            "\u001b[32m2024-08-06 22:35:32.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m107\u001b[0m - Done preprocessing installments_payments.\n",
            "\u001b[32m2024-08-06 22:35:32.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m108\u001b[0m - Initial Size of installments_payments: (13605401, 8)\n",
            "\u001b[32m2024-08-06 22:35:32.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m109\u001b[0m - Size of installments_payments after Pre-Processing, Feature Engineering and Aggregation: (339587, 7)\n",
            "\u001b[32m2024-08-06 22:35:32.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m110\u001b[0m - Total Time Taken: 0:00:21.271171\n"
          ]
        }
      ],
      "source": [
        "installments_aggregated = preprocess_installments_payments(file_directory = DIR).main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4UDARqsLfl2"
      },
      "source": [
        "## POS_CASH_balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL_uImpuLlHE"
      },
      "outputs": [],
      "source": [
        "class preprocess_POS_CASH_balance:\n",
        "    '''\n",
        "    Preprocess the POS_CASH_balance table.\n",
        "\n",
        "    This class contains methods to load, preprocess, and aggregate the `POS_CASH_balance` table.\n",
        "\n",
        "    Attributes:\n",
        "        file_directory (str): Path to the directory containing the data files.\n",
        "        verbose (bool): Whether to enable verbose logging.\n",
        "        dump_to_pickle (bool): Whether to pickle the final preprocessed table.\n",
        "        nrows (Optional[int]): Number of rows to read from the CSV file.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, file_directory: str = '', verbose: bool = True, dump_to_pickle: bool = False, nrows: Optional[int] = None):\n",
        "        '''\n",
        "        Initializes the preprocess_POS_CASH_balance class.\n",
        "\n",
        "        Args:\n",
        "            file_directory (str): Path to the directory where the files are located.\n",
        "            verbose (bool): Whether to enable verbose logging.\n",
        "            dump_to_pickle (bool): Whether to pickle the final preprocessed table.\n",
        "            nrows (Optional[int]): Number of rows to read from the CSV file.\n",
        "        '''\n",
        "        self.file_directory = file_directory\n",
        "        self.verbose = verbose\n",
        "        self.dump_to_pickle = dump_to_pickle\n",
        "        self.nrows = nrows\n",
        "        self.start = datetime.now()\n",
        "        logger.info('Preprocessing class initialized.')\n",
        "\n",
        "    def load_dataframe(self):\n",
        "        '''\n",
        "        Loads the `POS_CASH_balance.csv` DataFrame into memory.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            logger.info('#########################################################')\n",
        "            logger.info('#          Pre-processing POS_CASH_balance.csv          #')\n",
        "            logger.info('#########################################################')\n",
        "            logger.info(\"Loading the DataFrame, POS_CASH_balance.csv, into memory...\")\n",
        "\n",
        "        self.pos_cash = reduce_memory_usage(pd.read_csv(self.file_directory + 'POS_CASH_balance.csv', nrows=self.nrows))\n",
        "        self.initial_size = self.pos_cash.shape\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Loaded POS_CASH_balance.csv\")\n",
        "            logger.info('Time Taken to load: {}', datetime.now() - self.start)\n",
        "\n",
        "    def data_preprocessing_and_feature_engineering(self):\n",
        "        '''\n",
        "        Performs preprocessing and feature engineering on the DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            start = datetime.now()\n",
        "            logger.info(\"Starting Data Pre-processing and Feature Engineering...\")\n",
        "\n",
        "        # Example of preprocessing and feature engineering (customize as needed)\n",
        "        # self.pos_cash['new_feature'] = self.pos_cash['feature_1'] / self.pos_cash['feature_2']\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Data Pre-processing and Feature Engineering Done.\")\n",
        "            logger.info('Time Taken: {}', datetime.now() - start)\n",
        "\n",
        "    def aggregations_sk_id_curr(self) -> pd.DataFrame:\n",
        "        '''\n",
        "        Aggregates the POS_CASH_balance table over SK_ID_CURR.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: POS_CASH_balance table aggregated over SK_ID_CURR.\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            logger.info(\"Aggregating POS_CASH_balance over SK_ID_CURR...\")\n",
        "\n",
        "        # Combining numerical features\n",
        "        pos_cash_numerical_aggregated = self.pos_cash.select_dtypes(include=[np.number]).drop('SK_ID_PREV', axis=1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n",
        "\n",
        "        # Combining categorical features\n",
        "        pos_cash_categorical = pd.get_dummies(self.pos_cash.select_dtypes('object'))\n",
        "        pos_cash_categorical['SK_ID_CURR'] = self.pos_cash['SK_ID_CURR']\n",
        "        pos_cash_categorical_aggregated = pos_cash_categorical.groupby('SK_ID_CURR').mean().reset_index()\n",
        "\n",
        "        # Merge numerical and categorical features\n",
        "        pos_cash_aggregated = pos_cash_numerical_aggregated.merge(pos_cash_categorical_aggregated, on='SK_ID_CURR')\n",
        "        pos_cash_aggregated.columns = ['POS_' + column if column != 'SK_ID_CURR' else column for column in pos_cash_aggregated.columns]\n",
        "        pos_cash_aggregated.fillna(0, inplace=True)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info('Aggregation Done.')\n",
        "            logger.info('Size after aggregation: {}', pos_cash_aggregated.shape)\n",
        "\n",
        "        return pos_cash_aggregated\n",
        "\n",
        "    def main(self) -> pd.DataFrame:\n",
        "        '''\n",
        "        Performs complete preprocessing and aggregation of the `POS_CASH_balance` table.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Final preprocessed and aggregated `POS_CASH_balance` table.\n",
        "        '''\n",
        "        # Loading the DataFrame\n",
        "        self.load_dataframe()\n",
        "\n",
        "        # Performing preprocessing and feature engineering\n",
        "        self.data_preprocessing_and_feature_engineering()\n",
        "\n",
        "        # Aggregating the POS_CASH_balance over SK_ID_CURR\n",
        "        pos_cash_aggregated = self.aggregations_sk_id_curr()\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info('Done preprocessing POS_CASH_balance.')\n",
        "            logger.info('Initial Size of POS_CASH_balance: {}', self.initial_size)\n",
        "            logger.info('Size of POS_CASH_balance after Pre-Processing, Feature Engineering and Aggregation: {}', pos_cash_aggregated.shape)\n",
        "            logger.info('Total Time Taken: {}', datetime.now() - self.start)\n",
        "\n",
        "        if self.dump_to_pickle:\n",
        "            if self.verbose:\n",
        "                logger.info('Pickling pre-processed POS_CASH_balance to POS_CASH_balance_preprocessed.pkl')\n",
        "            with open(self.file_directory + 'POS_CASH_balance_preprocessed.pkl', 'wb') as f:\n",
        "                pickle.dump(pos_cash_aggregated, f)\n",
        "            if self.verbose:\n",
        "                logger.info('Pickling completed.')\n",
        "\n",
        "        return pos_cash_aggregated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pdp7fRsL-2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8d185cf-4b2f-4d93-ced4-19983bf605b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2024-08-06 22:35:32.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m29\u001b[0m - Preprocessing class initialized.\n",
            "\u001b[32m2024-08-06 22:35:32.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m39\u001b[0m - #########################################################\n",
            "\u001b[32m2024-08-06 22:35:32.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m40\u001b[0m - #          Pre-processing POS_CASH_balance.csv          #\n",
            "\u001b[32m2024-08-06 22:35:32.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m41\u001b[0m - #########################################################\n",
            "\u001b[32m2024-08-06 22:35:32.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m42\u001b[0m - Loading the DataFrame, POS_CASH_balance.csv, into memory...\n",
            "\u001b[32m2024-08-06 22:35:43.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m24\u001b[0m - Memory usage of dataframe is 610.43 MB\n",
            "\u001b[32m2024-08-06 22:35:43.624\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m55\u001b[0m - Memory usage after optimization is: 238.45 MB\n",
            "\u001b[32m2024-08-06 22:35:43.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m56\u001b[0m - Decreased by 60.9%\n",
            "\u001b[32m2024-08-06 22:35:43.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m48\u001b[0m - Loaded POS_CASH_balance.csv\n",
            "\u001b[32m2024-08-06 22:35:43.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m49\u001b[0m - Time Taken to load: 0:00:11.260503\n",
            "\u001b[32m2024-08-06 22:35:43.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_preprocessing_and_feature_engineering\u001b[0m:\u001b[36m60\u001b[0m - Starting Data Pre-processing and Feature Engineering...\n",
            "\u001b[32m2024-08-06 22:35:43.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_preprocessing_and_feature_engineering\u001b[0m:\u001b[36m66\u001b[0m - Data Pre-processing and Feature Engineering Done.\n",
            "\u001b[32m2024-08-06 22:35:43.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_preprocessing_and_feature_engineering\u001b[0m:\u001b[36m67\u001b[0m - Time Taken: 0:00:00.004075\n",
            "\u001b[32m2024-08-06 22:35:43.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36maggregations_sk_id_curr\u001b[0m:\u001b[36m77\u001b[0m - Aggregating POS_CASH_balance over SK_ID_CURR...\n",
            "\u001b[32m2024-08-06 22:35:49.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36maggregations_sk_id_curr\u001b[0m:\u001b[36m93\u001b[0m - Aggregation Done.\n",
            "\u001b[32m2024-08-06 22:35:49.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36maggregations_sk_id_curr\u001b[0m:\u001b[36m94\u001b[0m - Size after aggregation: (337252, 15)\n",
            "\u001b[32m2024-08-06 22:35:49.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m115\u001b[0m - Done preprocessing POS_CASH_balance.\n",
            "\u001b[32m2024-08-06 22:35:49.647\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m116\u001b[0m - Initial Size of POS_CASH_balance: (10001358, 8)\n",
            "\u001b[32m2024-08-06 22:35:49.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m117\u001b[0m - Size of POS_CASH_balance after Pre-Processing, Feature Engineering and Aggregation: (337252, 15)\n",
            "\u001b[32m2024-08-06 22:35:49.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m118\u001b[0m - Total Time Taken: 0:00:17.282258\n"
          ]
        }
      ],
      "source": [
        "pos_aggregated = preprocess_POS_CASH_balance(file_directory = DIR).main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqwtd7CjNVE-"
      },
      "source": [
        "## credit_card_balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQZizAVpNayR"
      },
      "outputs": [],
      "source": [
        "class preprocess_credit_card_balance:\n",
        "    '''\n",
        "    Preprocess the credit_card_balance table.\n",
        "\n",
        "    This class contains methods to load, preprocess, and aggregate the `credit_card_balance` table.\n",
        "\n",
        "    Attributes:\n",
        "        file_directory (str): Path to the directory containing the data files.\n",
        "        verbose (bool): Whether to enable verbose logging.\n",
        "        dump_to_pickle (bool): Whether to pickle the final preprocessed table.\n",
        "        nrows (Optional[int]): Number of rows to read from the CSV file.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, file_directory: str = '', verbose: bool = True, dump_to_pickle: bool = False, nrows: Optional[int] = None):\n",
        "        '''\n",
        "        Initializes the preprocess_credit_card_balance class.\n",
        "\n",
        "        Args:\n",
        "            file_directory (str): Path to the directory where the files are located.\n",
        "            verbose (bool): Whether to enable verbose logging.\n",
        "            dump_to_pickle (bool): Whether to pickle the final preprocessed table.\n",
        "            nrows (Optional[int]): Number of rows to read from the CSV file.\n",
        "        '''\n",
        "        self.file_directory = file_directory\n",
        "        self.verbose = verbose\n",
        "        self.dump_to_pickle = dump_to_pickle\n",
        "        self.nrows = nrows\n",
        "        self.start = datetime.now()\n",
        "        logger.info('Preprocessing class initialized.')\n",
        "\n",
        "    def load_dataframe(self):\n",
        "        '''\n",
        "        Loads the `credit_card_balance.csv` DataFrame into memory.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            logger.info('#########################################################')\n",
        "            logger.info('#        Pre-processing credit_card_balance.csv         #')\n",
        "            logger.info('#########################################################')\n",
        "            logger.info(\"Loading the DataFrame, credit_card_balance.csv, into memory...\")\n",
        "\n",
        "        self.cc_balance = pd.read_csv(self.file_directory + 'credit_card_balance.csv', nrows=self.nrows)\n",
        "        self.initial_size = self.cc_balance.shape\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Loaded credit_card_balance.csv\")\n",
        "            logger.info('Time Taken to load: {}', datetime.now() - self.start)\n",
        "\n",
        "    def data_preprocessing_and_feature_engineering(self):\n",
        "        '''\n",
        "        Performs preprocessing and feature engineering on the DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            start = datetime.now()\n",
        "            logger.info(\"Starting Data Pre-processing and Feature Engineering...\")\n",
        "\n",
        "        # Example of preprocessing and feature engineering (customize as needed)\n",
        "        # self.cc_balance['new_feature'] = self.cc_balance['feature_1'] / self.cc_balance['feature_2']\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Data Pre-processing and Feature Engineering Done.\")\n",
        "            logger.info('Time Taken: {}', datetime.now() - start)\n",
        "\n",
        "    def aggregations(self) -> pd.DataFrame:\n",
        "        '''\n",
        "        Aggregates the `credit_card_balance` table first over `SK_ID_PREV`, and then over `SK_ID_CURR`.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Aggregated `credit_card_balance` table.\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            logger.info(\"Aggregating the DataFrame, first over SK_ID_PREV, then over SK_ID_CURR\")\n",
        "\n",
        "        # Combining numerical features\n",
        "        cc_numerical_aggregated = self.cc_balance.select_dtypes(include=[np.number]).drop('SK_ID_PREV', axis=1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n",
        "\n",
        "        # Combining categorical features\n",
        "        cc_categorical = pd.get_dummies(self.cc_balance.select_dtypes('object'))\n",
        "        cc_categorical['SK_ID_CURR'] = self.cc_balance['SK_ID_CURR']\n",
        "        cc_categorical_aggregated = cc_categorical.groupby('SK_ID_CURR').mean().reset_index()\n",
        "\n",
        "        # Merge numerical and categorical features\n",
        "        cc_aggregated = cc_numerical_aggregated.merge(cc_categorical_aggregated, on='SK_ID_CURR')\n",
        "        cc_aggregated.columns = ['CC_' + column if column != 'SK_ID_CURR' else column for column in cc_aggregated.columns]\n",
        "        cc_aggregated.fillna(0, inplace=True)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info('Aggregation Done.')\n",
        "            logger.info('Size after aggregation: {}', cc_aggregated.shape)\n",
        "\n",
        "        return cc_aggregated\n",
        "\n",
        "    def main(self) -> pd.DataFrame:\n",
        "        '''\n",
        "        Performs complete preprocessing and aggregation of the `credit_card_balance` table.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Final preprocessed and aggregated `credit_card_balance` table.\n",
        "        '''\n",
        "        # Loading the DataFrame\n",
        "        self.load_dataframe()\n",
        "\n",
        "        # Performing preprocessing and feature engineering\n",
        "        self.data_preprocessing_and_feature_engineering()\n",
        "\n",
        "        # Aggregating the `credit_card_balance` over SK_ID_PREV and SK_ID_CURR\n",
        "        cc_aggregated = self.aggregations()\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info('Done preprocessing credit_card_balance.')\n",
        "            logger.info('Initial Size of credit_card_balance: {}', self.initial_size)\n",
        "            logger.info('Size of credit_card_balance after Pre-Processing, Feature Engineering and Aggregation: {}', cc_aggregated.shape)\n",
        "            logger.info('Total Time Taken: {}', datetime.now() - self.start)\n",
        "\n",
        "        if self.dump_to_pickle:\n",
        "            if self.verbose:\n",
        "                logger.info('Pickling pre-processed credit_card_balance to credit_card_balance_preprocessed.pkl')\n",
        "            with open(self.file_directory + 'credit_card_balance_preprocessed.pkl', 'wb') as f:\n",
        "                pickle.dump(cc_aggregated, f)\n",
        "            if self.verbose:\n",
        "                logger.info('Pickling completed.')\n",
        "\n",
        "        return cc_aggregated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBPV2L2ENo2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5bd26ce-d943-4294-b362-a20e5e256389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2024-08-06 22:35:49.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m29\u001b[0m - Preprocessing class initialized.\n",
            "\u001b[32m2024-08-06 22:35:49.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m39\u001b[0m - #########################################################\n",
            "\u001b[32m2024-08-06 22:35:49.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m40\u001b[0m - #        Pre-processing credit_card_balance.csv         #\n",
            "\u001b[32m2024-08-06 22:35:49.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m41\u001b[0m - #########################################################\n",
            "\u001b[32m2024-08-06 22:35:49.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m42\u001b[0m - Loading the DataFrame, credit_card_balance.csv, into memory...\n",
            "\u001b[32m2024-08-06 22:36:03.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m48\u001b[0m - Loaded credit_card_balance.csv\n",
            "\u001b[32m2024-08-06 22:36:03.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframe\u001b[0m:\u001b[36m49\u001b[0m - Time Taken to load: 0:00:14.182208\n",
            "\u001b[32m2024-08-06 22:36:03.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_preprocessing_and_feature_engineering\u001b[0m:\u001b[36m60\u001b[0m - Starting Data Pre-processing and Feature Engineering...\n",
            "\u001b[32m2024-08-06 22:36:03.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_preprocessing_and_feature_engineering\u001b[0m:\u001b[36m66\u001b[0m - Data Pre-processing and Feature Engineering Done.\n",
            "\u001b[32m2024-08-06 22:36:03.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_preprocessing_and_feature_engineering\u001b[0m:\u001b[36m67\u001b[0m - Time Taken: 0:00:00.005486\n",
            "\u001b[32m2024-08-06 22:36:03.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36maggregations\u001b[0m:\u001b[36m77\u001b[0m - Aggregating the DataFrame, first over SK_ID_PREV, then over SK_ID_CURR\n",
            "\u001b[32m2024-08-06 22:36:07.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36maggregations\u001b[0m:\u001b[36m93\u001b[0m - Aggregation Done.\n",
            "\u001b[32m2024-08-06 22:36:07.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36maggregations\u001b[0m:\u001b[36m94\u001b[0m - Size after aggregation: (103558, 28)\n",
            "\u001b[32m2024-08-06 22:36:07.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m115\u001b[0m - Done preprocessing credit_card_balance.\n",
            "\u001b[32m2024-08-06 22:36:07.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m116\u001b[0m - Initial Size of credit_card_balance: (3840312, 23)\n",
            "\u001b[32m2024-08-06 22:36:07.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m117\u001b[0m - Size of credit_card_balance after Pre-Processing, Feature Engineering and Aggregation: (103558, 28)\n",
            "\u001b[32m2024-08-06 22:36:07.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m118\u001b[0m - Total Time Taken: 0:00:17.327058\n"
          ]
        }
      ],
      "source": [
        "cc_aggregated = preprocess_credit_card_balance(file_directory=DIR).main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fVmBYfWN74r"
      },
      "source": [
        "## application_train and application_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QelkXBP4N9si"
      },
      "outputs": [],
      "source": [
        "class preprocess_application_train_test:\n",
        "    '''\n",
        "    Preprocess the application_train and application_test tables.\n",
        "    Contains 4 member functions:\n",
        "        1. init method\n",
        "        2. load_dataframes method\n",
        "        3. data_cleaning method\n",
        "        4. main method\n",
        "    '''\n",
        "\n",
        "    def __init__(self, file_directory1='', file_directory2='', verbose=True, dump_to_pickle=False):\n",
        "        '''\n",
        "        Initialize the class members.\n",
        "\n",
        "        Args:\n",
        "            file_directory1: str, default=''\n",
        "                Path where the application_train.csv file exists. Include a '/' at the end.\n",
        "            file_directory2: str, default=''\n",
        "                Path where the application_test.csv file exists. Include a '/' at the end.\n",
        "            verbose: bool, default=True\n",
        "                Whether to enable verbosity or not.\n",
        "            dump_to_pickle: bool, default=False\n",
        "                Whether to pickle the final preprocessed tables or not.\n",
        "\n",
        "        '''\n",
        "        self.verbose = verbose\n",
        "        self.dump_to_pickle = dump_to_pickle\n",
        "        self.file_directory1 = file_directory1\n",
        "        self.file_directory2 = file_directory2\n",
        "\n",
        "    def load_dataframes(self):\n",
        "        '''\n",
        "        Load the application_train.csv and application_test.csv DataFrames.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            self.start = datetime.now()\n",
        "            logger.info('#######################################################')\n",
        "            logger.info('#        Pre-processing application_train.csv         #')\n",
        "            logger.info('#        Pre-processing application_test.csv          #')\n",
        "            logger.info('#######################################################')\n",
        "            logger.info(\"\\nLoading the DataFrames into memory...\")\n",
        "\n",
        "        self.application_train = reduce_memory_usage(pd.read_csv(self.file_directory1 + 'application_train.csv'))\n",
        "        self.application_test = reduce_memory_usage(pd.read_csv(self.file_directory2 + 'application_test.csv'))\n",
        "        self.initial_train_shape = self.application_train.shape\n",
        "        self.initial_test_shape = self.application_test.shape\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Loaded application_train.csv and application_test.csv\")\n",
        "            logger.info(f\"Time Taken to load = {datetime.now() - self.start}\")\n",
        "\n",
        "    def data_cleaning(self):\n",
        "        '''\n",
        "        Clean the tables by removing erroneous rows/entries.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "        if self.verbose:\n",
        "            logger.info(\"\\nPerforming Data Cleaning...\")\n",
        "        #there are some FLAG_DOCUMENT features having just one category for almost all data, we will remove those\n",
        "        flag_cols_to_drop = ['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_20']\n",
        "\n",
        "        self.application_train.drop(flag_cols_to_drop, axis=1, inplace=True)\n",
        "        self.application_test.drop(flag_cols_to_drop, axis=1, inplace=True)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Data Cleaning Done.\")\n",
        "\n",
        "    def main(self):\n",
        "        '''\n",
        "        Complete preprocessing of application_train and application_test tables.\n",
        "\n",
        "        Returns:\n",
        "            Final preprocessed application_train and application_test tables.\n",
        "        '''\n",
        "        # Load the DataFrames\n",
        "        self.load_dataframes()\n",
        "\n",
        "        # Perform Data Cleaning\n",
        "        self.data_cleaning()\n",
        "\n",
        "        # Feature Engineering\n",
        "        if self.verbose:\n",
        "            start = datetime.now()\n",
        "            logger.info(\"\\nStarting Feature Engineering...\")\n",
        "            logger.info(\"\\nCreating Domain Based Features on Numeric Data\")\n",
        "\n",
        "        cat_col_train = [category for category in self.application_train.columns if self.application_train[category].dtype == 'object']\n",
        "        cat_col_test = [category for category in self.application_test.columns if self.application_test[category].dtype == 'object']\n",
        "\n",
        "        self.application_train = pd.get_dummies(self.application_train, columns=cat_col_train)\n",
        "        self.application_test = pd.get_dummies(self.application_test, columns=cat_col_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(\"Creating features based on Categorical Interactions on some Numeric Features\")\n",
        "            logger.info(\"Feature Engineering Done.\")\n",
        "            logger.info(f\"Time taken = {datetime.now() - start}\")\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info('Preprocessing Done.')\n",
        "            logger.info(f\"\\nInitial Size of application_train: {self.initial_train_shape}\")\n",
        "            logger.info(f\"Size of application_train after Pre-Processing and Feature Engineering: {self.application_train.shape}\")\n",
        "            logger.info(f\"\\nInitial Size of application_test: {self.initial_test_shape}\")\n",
        "            logger.info(f\"Size of application_test after Pre-Processing and Feature Engineering: {self.application_test.shape}\")\n",
        "            logger.info(f'\\nTotal Time Taken = {datetime.now() - self.start}')\n",
        "\n",
        "        if self.dump_to_pickle:\n",
        "            if self.verbose:\n",
        "                logger.info('\\nPickling pre-processed application_train and application_test to pickle files.')\n",
        "            with open(self.file_directory1 + 'application_train_preprocessed.pkl', 'wb') as f:\n",
        "                pickle.dump(self.application_train, f)\n",
        "            with open(self.file_directory2 + 'application_test_preprocessed.pkl', 'wb') as f:\n",
        "                pickle.dump(self.application_test, f)\n",
        "            if self.verbose:\n",
        "                logger.info('Pickling Done.')\n",
        "        if self.verbose:\n",
        "            logger.info('-'*100)\n",
        "\n",
        "        return self.application_train, self.application_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV8qiwDDPE8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb1cda7-5f15-4137-97fd-c80f975391cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2024-08-06 22:36:07.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframes\u001b[0m:\u001b[36m40\u001b[0m - #######################################################\n",
            "\u001b[32m2024-08-06 22:36:07.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframes\u001b[0m:\u001b[36m41\u001b[0m - #        Pre-processing application_train.csv         #\n",
            "\u001b[32m2024-08-06 22:36:07.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframes\u001b[0m:\u001b[36m42\u001b[0m - #        Pre-processing application_test.csv          #\n",
            "\u001b[32m2024-08-06 22:36:07.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframes\u001b[0m:\u001b[36m43\u001b[0m - #######################################################\n",
            "\u001b[32m2024-08-06 22:36:07.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframes\u001b[0m:\u001b[36m44\u001b[0m - \n",
            "Loading the DataFrames into memory...\n",
            "\u001b[32m2024-08-06 22:36:14.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m24\u001b[0m - Memory usage of dataframe is 286.23 MB\n",
            "\u001b[32m2024-08-06 22:36:14.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m55\u001b[0m - Memory usage after optimization is: 91.79 MB\n",
            "\u001b[32m2024-08-06 22:36:14.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m56\u001b[0m - Decreased by 67.9%\n",
            "\u001b[32m2024-08-06 22:36:15.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m24\u001b[0m - Memory usage of dataframe is 45.00 MB\n",
            "\u001b[32m2024-08-06 22:36:16.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m55\u001b[0m - Memory usage after optimization is: 14.60 MB\n",
            "\u001b[32m2024-08-06 22:36:16.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m56\u001b[0m - Decreased by 67.6%\n",
            "\u001b[32m2024-08-06 22:36:16.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframes\u001b[0m:\u001b[36m52\u001b[0m - Loaded application_train.csv and application_test.csv\n",
            "\u001b[32m2024-08-06 22:36:16.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataframes\u001b[0m:\u001b[36m53\u001b[0m - Time Taken to load = 0:00:09.016394\n",
            "\u001b[32m2024-08-06 22:36:16.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_cleaning\u001b[0m:\u001b[36m63\u001b[0m - \n",
            "Performing Data Cleaning...\n",
            "\u001b[32m2024-08-06 22:36:16.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdata_cleaning\u001b[0m:\u001b[36m71\u001b[0m - Data Cleaning Done.\n",
            "\u001b[32m2024-08-06 22:36:16.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m89\u001b[0m - \n",
            "Starting Feature Engineering...\n",
            "\u001b[32m2024-08-06 22:36:16.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m90\u001b[0m - \n",
            "Creating Domain Based Features on Numeric Data\n",
            "\u001b[32m2024-08-06 22:36:17.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m99\u001b[0m - Creating features based on Categorical Interactions on some Numeric Features\n",
            "\u001b[32m2024-08-06 22:36:17.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m100\u001b[0m - Feature Engineering Done.\n",
            "\u001b[32m2024-08-06 22:36:17.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m101\u001b[0m - Time taken = 0:00:00.867705\n",
            "\u001b[32m2024-08-06 22:36:17.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m104\u001b[0m - Preprocessing Done.\n",
            "\u001b[32m2024-08-06 22:36:17.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m105\u001b[0m - \n",
            "Initial Size of application_train: (307511, 122)\n",
            "\u001b[32m2024-08-06 22:36:17.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m106\u001b[0m - Size of application_train after Pre-Processing and Feature Engineering: (307511, 240)\n",
            "\u001b[32m2024-08-06 22:36:17.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m107\u001b[0m - \n",
            "Initial Size of application_test: (48744, 121)\n",
            "\u001b[32m2024-08-06 22:36:17.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m108\u001b[0m - Size of application_test after Pre-Processing and Feature Engineering: (48744, 237)\n",
            "\u001b[32m2024-08-06 22:36:17.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m109\u001b[0m - \n",
            "Total Time Taken = 0:00:10.084003\n",
            "\u001b[32m2024-08-06 22:36:17.227\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m121\u001b[0m - ----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "application_train, application_test = preprocess_application_train_test(file_directory1=CLEANED_DATA_DIR, file_directory2=DIR).main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5TsVSqA_yCT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def merge_all_tables(application_train, application_test, bureau_aggregated, previous_aggregated,\n",
        "                    installments_aggregated, pos_aggregated, cc_aggregated):\n",
        "    '''\n",
        "    Function to merge all the tables together with the application_train and application_test tables\n",
        "    on SK_ID_CURR.\n",
        "\n",
        "    Inputs:\n",
        "        All the previously pre-processed Tables.\n",
        "\n",
        "    Returns:\n",
        "        Single merged tables, one for training data and one for test data\n",
        "    '''\n",
        "    logger.info(\"Merging application_train and application_test with aggregated tables.\")\n",
        "\n",
        "    #merging application_train and application_test with Aggregated bureau table\n",
        "    app_train_merged = application_train.merge(bureau_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
        "    app_test_merged = application_test.merge(bureau_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
        "    logger.info(\"Merged with bureau_aggregated.\")\n",
        "\n",
        "    #merging with aggregated previous_applications\n",
        "    app_train_merged = app_train_merged.merge(previous_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
        "    app_test_merged = app_test_merged.merge(previous_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
        "    logger.info(\"Merged with previous_aggregated.\")\n",
        "\n",
        "    #merging with aggregated installments tables\n",
        "    app_train_merged = app_train_merged.merge(installments_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
        "    app_test_merged = app_test_merged.merge(installments_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
        "    logger.info(\"Merged with installments_aggregated.\")\n",
        "\n",
        "    #merging with aggregated POS_Cash balance table\n",
        "    app_train_merged = app_train_merged.merge(pos_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
        "    app_test_merged = app_test_merged.merge(pos_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
        "    logger.info(\"Merged with pos_aggregated.\")\n",
        "\n",
        "    #merging with aggregated credit card table\n",
        "    app_train_merged = app_train_merged.merge(cc_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
        "    app_test_merged = app_test_merged.merge(cc_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
        "    logger.info(\"Merged with cc_aggregated.\")\n",
        "\n",
        "    # Filling missing values with 0\n",
        "    app_train_merged = app_train_merged.fillna(0)\n",
        "    app_test_merged = app_test_merged.fillna(0)\n",
        "    logger.info(\"Filled missing values with 0.\")\n",
        "\n",
        "    # Clean column names to remove non-alphanumeric characters\n",
        "    app_train_merged = app_train_merged.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "    app_test_merged = app_test_merged.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "    logger.info(\"Cleaned column names.\")\n",
        "\n",
        "    # Ensure the columns are the same for train and test data\n",
        "    train_cols = set(app_train_merged.columns)\n",
        "    test_cols = set(app_test_merged.columns)\n",
        "\n",
        "    diff_train_cols = train_cols.difference(test_cols)\n",
        "    diff_test_cols = test_cols.difference(train_cols)\n",
        "\n",
        "    if 'TARGET' in diff_train_cols:\n",
        "        diff_train_cols.remove('TARGET')\n",
        "\n",
        "    app_train_merged.drop(diff_train_cols, axis=1, inplace=True)\n",
        "    app_test_merged.drop(diff_test_cols, axis=1, inplace=True)\n",
        "    logger.info(\"Ensured train and test data have the same columns.\")\n",
        "\n",
        "\n",
        "    #removing the SK_ID_CURR from training  data\n",
        "    app_train_merged = app_train_merged.drop(['SK_ID_CURR'], axis = 1)\n",
        "    logger.info(\"Removed SK_ID_CURR from the data.\")\n",
        "\n",
        "    return reduce_memory_usage(app_train_merged), reduce_memory_usage(app_test_merged)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = merge_all_tables(application_train, application_test, bureau_aggregated, previous_aggregated,\n",
        "                    installments_aggregated, pos_aggregated, cc_aggregated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8-Y-dviiutz",
        "outputId": "8c1289f9-95ca-4c14-baf4-a5bfcfa04a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2024-08-06 22:48:55.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m24\u001b[0m - Memory usage of dataframe is 650.17 MB\n",
            "\u001b[32m2024-08-06 22:48:58.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m55\u001b[0m - Memory usage after optimization is: 295.03 MB\n",
            "\u001b[32m2024-08-06 22:48:58.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m56\u001b[0m - Decreased by 54.6%\n",
            "\u001b[32m2024-08-06 22:48:58.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m24\u001b[0m - Memory usage of dataframe is 103.29 MB\n",
            "\u001b[32m2024-08-06 22:48:59.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m55\u001b[0m - Memory usage after optimization is: 47.00 MB\n",
            "\u001b[32m2024-08-06 22:48:59.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_memory_usage\u001b[0m:\u001b[36m56\u001b[0m - Decreased by 54.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to_csv(DIR + 'train_data_final.csv')\n",
        "test_data.to_csv(DIR + 'test_data_final.csv')"
      ],
      "metadata": {
        "id": "Icf0uxjVse4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c home-credit-default-risk"
      ],
      "metadata": {
        "id": "Tvnc0kdtFJuu",
        "outputId": "c44c6587-09f7-409a-8e97-c9b5df34338e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 7, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 407, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FXkWDUDbFKew"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}